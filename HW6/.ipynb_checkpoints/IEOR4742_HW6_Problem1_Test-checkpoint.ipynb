{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From <ipython-input-1-0dcabbfe7b0e>:98: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Restoring parameters from D:\\JupyterNotebook\\IEOR_4742\\HW6\\bAbI-model/model.ckpt\n",
      "indefinite knowledge ... 55.377% Error Rate.\n",
      "basic coreference ... 35.700% Error Rate.\n",
      "conjunction ... 31.000% Error Rate.\n",
      "compound coreference ... 12.200% Error Rate.\n",
      "time reasoning ... 80.400% Error Rate.\n",
      "basic deduction ... 79.421% Error Rate.\n",
      "basic induction ... 73.081% Error Rate.\n",
      "positional reasoning ... 53.474% Error Rate.\n",
      "size reasoning ... 39.602% Error Rate.\n",
      "path finding ... 91.633% Error Rate.\n",
      "single supporting fact ... 52.510% Error Rate.\n",
      "agents motivations ... 9.719% Error Rate.\n",
      "two supporting facts ... 78.879% Error Rate.\n",
      "three supporting facts ... 81.700% Error Rate.\n",
      "two arg relations ... 58.466% Error Rate.\n",
      "three arg relations ... 66.867% Error Rate.\n",
      "yes no questions ... 49.300% Error Rate.\n",
      "counting ... 51.200% Error Rate.\n",
      "lists sets ... 65.500% Error Rate.\n",
      "simple negation ... 59.800% Error Rate.\n",
      "\n",
      "\n",
      "Task                       Result                     Paper's Mean\n",
      "-------------------------------------------------------------------\n",
      "single supporting fact     52.51%                     9.0 ± 12.6%\n",
      "two supporting facts       78.88%                     39.2 ± 20.5%\n",
      "three supporting facts     81.70%                     39.6 ± 16.4%\n",
      "two arg relations          58.47%                     0.4 ± 0.7%\n",
      "three arg relations        66.87%                     1.5 ± 1.0%\n",
      "yes no questions           49.30%                     6.9 ± 7.5%\n",
      "counting                   51.20%                     9.8 ± 7.0%\n",
      "lists sets                 65.50%                     5.5 ± 5.9%\n",
      "simple negation            59.80%                     7.7 ± 8.3%\n",
      "indefinite knowledge       55.38%                     9.6 ± 11.4%\n",
      "basic coreference          35.70%                     3.3 ± 5.7%\n",
      "conjunction                31.00%                     5.0 ± 6.3%\n",
      "compound coreference       12.20%                     3.1 ± 3.6%\n",
      "time reasoning             80.40%                     11.0 ± 7.5%\n",
      "basic deduction            79.42%                     27.2 ± 20.1%\n",
      "basic induction            73.08%                     53.6 ± 1.9%\n",
      "positional reasoning       53.47%                     32.4 ± 8.0%\n",
      "size reasoning             39.60%                     4.2 ± 1.8%\n",
      "path finding               91.63%                     64.6 ± 37.4%\n",
      "agents motivations         9.72%                      0.0 ± 0.1%\n",
      "-------------------------------------------------------------------\n",
      "Mean Err.                  56.29%                     16.7 ± 7.6%\n",
      "Failed (err. > 5%)         20                         11.2 ± 5.4\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "#import getopt\n",
    "#import shutil\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import memory_operations as op\n",
    "\n",
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def load(path):\n",
    "    return pickle.load(open(path, 'rb'))\n",
    "\n",
    "def onehot(index, size):\n",
    "    vec = np.zeros(size, dtype=np.float32)\n",
    "    index = int(index)\n",
    "    vec[index] = 1.0\n",
    "    return vec\n",
    "\n",
    "def prepare_sample(sample, target_code, word_space_size):\n",
    "\n",
    "    \"\"\"\n",
    "    prepares the input/output sequence of a sample story by encoding it\n",
    "    into one-hot vectors and generates the necessary loss weights\n",
    "    \"\"\"\n",
    "\t\n",
    "    input_vec = np.array(sample[0]['inputs'], dtype=np.float32)\n",
    "    output_vec = np.array(sample[0]['inputs'], dtype=np.float32)\n",
    "    seq_len = input_vec.shape[0]\n",
    "    weights_vec = np.zeros(seq_len, dtype=np.float32)\n",
    "\n",
    "    target_mask = (input_vec == target_code)\n",
    "    output_vec[target_mask] = sample[0]['outputs']\n",
    "    weights_vec[target_mask] = 1.0\n",
    "\n",
    "    input_vec = np.array([onehot(code, word_space_size) for code in input_vec])\n",
    "    output_vec = np.array([onehot(code, word_space_size) for code in output_vec])\n",
    "\n",
    "    return (\n",
    "        np.reshape(input_vec, (-1, word_space_size)),\n",
    "        np.reshape(output_vec, (-1, word_space_size)),\n",
    "        seq_len,\n",
    "        np.reshape(weights_vec, (-1, 1))\n",
    "    )\n",
    "\n",
    "# task_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#get the current path of our code\n",
    "task_dir = Path().resolve()\n",
    "lexicon_dictionary = load(os.path.join(task_dir, \"data/en-10k/lexicon-dict.pkl\"))\n",
    "test_files = []\n",
    "\n",
    "test_dir = os.path.join(task_dir, 'data/en-10k/test/')\n",
    "for entryname in os.listdir(test_dir):\n",
    "    entry_path = os.path.join(test_dir, entryname)\n",
    "    if os.path.isfile(entry_path):\n",
    "        test_files.append(entry_path)\n",
    "\n",
    "# the model parameters\n",
    "N = 128   #256\n",
    "W = 32    #64 \n",
    "R = 2     #4     # memory parameters\n",
    "X = 159\n",
    "Y = 159   # input/output size\n",
    "NN = 256  # controller's network output size\n",
    "zeta_size = R*W + 3*W + 5*R + 3\n",
    "\n",
    "def network(step_input, state):\n",
    "    \n",
    "    \"\"\"\n",
    "    defines the recurrent neural network operation\n",
    "    \"\"\"\n",
    "    global NN\n",
    "    step_input = tf.expand_dims(step_input, 0)\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(NN)\n",
    "\n",
    "    return lstm_cell(step_input, state)\n",
    "\n",
    "\n",
    "# START: Computational Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # placeholders\n",
    "    input_data = tf.placeholder(tf.float32, [None, X])\n",
    "    sequence_length = tf.placeholder(tf.int32)\n",
    "\n",
    "    initial_nn_state = tf.nn.rnn_cell.BasicLSTMCell(NN).zero_state(1, tf.float32)\n",
    "\n",
    "    empty_unpacked_inputs = tf.TensorArray(tf.float32, sequence_length)\n",
    "    unpacked_inputs = empty_unpacked_inputs.unstack(input_data)\n",
    "    outputs_container = tf.TensorArray(tf.float32, sequence_length)  # accumelates the step outputs\n",
    "    t = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "    def step_op(time, memory_state, controller_state, inputs, outputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines the operation of one step of the sequence\n",
    "        \"\"\"\n",
    "        global N, W, R\n",
    "\n",
    "        step_input = inputs.read(time)\n",
    "        M, u, p, L, wr, ww, r = memory_state\n",
    "\n",
    "        with tf.variable_scope('controller'):\n",
    "            \n",
    "            std = lambda input_size: np.minimum(0.01, np.sqrt(2. / input_size))\n",
    "            \n",
    "            Xt = tf.concat(values=[step_input, tf.reshape(r, [-1])],axis=0,)\n",
    "\n",
    "            nn_output, nn_state = network(Xt, controller_state)\n",
    "\n",
    "            #std = lambda input_size: np.min(0.01, np.sqrt(2. / input_size))\n",
    "\n",
    "            W_zeta = tf.get_variable('W_zeta', [NN, zeta_size], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
    "            #W_zeta = tf.get_variable('W_zeta', [NN, zeta_size], tf.truncated_normal_initializer(stddev=std(NN)))\n",
    "\n",
    "            W_y = tf.get_variable('W_y', [NN, Y], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
    "            #W_zeta = tf.get_variable('W_zeta', [NN, zeta_size], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
    "\n",
    "            pre_output = tf.matmul(nn_output, W_y)\n",
    "            zeta = tf.squeeze(tf.matmul(nn_output, W_zeta))\n",
    "            kr, br, kw, bw, e, v, f, ga, gw, pi = op.parse_interface(zeta, N, W, R)\n",
    "\n",
    "            # write head operations\n",
    "            u_t = op.ut(u, f, wr, ww)\n",
    "            a_t = op.at(u_t, N)\n",
    "            cw_t = op.C(M, kw, bw)\n",
    "            ww_t = op.wwt(cw_t, a_t, gw, ga)\n",
    "            M_t = op.Mt(M, ww_t, e, v)\n",
    "            L_t = op.Lt(L, ww_t, p, N)\n",
    "            p_t = op.pt(ww_t, p)\n",
    "\n",
    "            # read heads operations\n",
    "            cr_t = op.C(M_t, kr, br)\n",
    "            wr_t = op.wrt(wr, L_t, cr_t, pi)\n",
    "            r_t = op.rt(M_t, wr_t)\n",
    "\n",
    "            W_r = tf.get_variable('W_r', [W*R, Y], tf.float32, tf.truncated_normal_initializer(stddev=std(W*R)))\n",
    "            flat_rt = tf.reshape(r_t, [-1])\n",
    "            final_output = pre_output + tf.matmul(tf.expand_dims(flat_rt, 0), W_r)\n",
    "            updated_outputs = outputs.write(time, tf.squeeze(final_output))\n",
    "\n",
    "            return time + 1, (M_t, u_t, p_t, L_t, wr_t, ww_t, r_t), nn_state, inputs, updated_outputs\n",
    "\n",
    "    _, _, _, _, final_outputs = tf.while_loop(\n",
    "        cond = lambda time, *_: time < sequence_length,\n",
    "        body = step_op,\n",
    "        loop_vars=(t, op.init_memory(N,W,R), initial_nn_state, unpacked_inputs, outputs_container),\n",
    "        parallel_iterations=32,\n",
    "        swap_memory=True\n",
    "    )\n",
    "\n",
    "    # pack the individual steps outputs into a single (sequence_length x Y) tensor\n",
    "    packed_output = final_outputs.stack()\n",
    "    softmaxed = tf.nn.softmax(packed_output)\n",
    "\n",
    "# END: Computational Graph\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "\n",
    "        tf.train.Saver().restore(session, os.path.join(task_dir, 'bAbI-model/model.ckpt'))\n",
    "\n",
    "        tasks_results = {}\n",
    "        tasks_names = {}\n",
    "        for test_file in test_files:\n",
    "            test_data = load(test_file)\n",
    "            task_regexp = r'qa([0-9]{1,2})_([a-z\\-]*)_test.txt.pkl'\n",
    "            task_filename = os.path.basename(test_file)\n",
    "            task_match_obj = re.match(task_regexp, task_filename)\n",
    "            task_number = task_match_obj.group(1)\n",
    "            task_name = task_match_obj.group(2).replace('-', ' ')\n",
    "            tasks_names[task_number] = task_name\n",
    "            counter = 0\n",
    "            results = []\n",
    "\n",
    "            llprint(\"%s ... %d/%d\" % (task_name, counter, len(test_data)))\n",
    "\n",
    "            for story in test_data:\n",
    "                astory = np.array(story['inputs'])\n",
    "                questions_indecies = np.argwhere(astory == lexicon_dictionary['?'])\n",
    "                questions_indecies = np.reshape(questions_indecies, (-1,))\n",
    "                target_mask = (astory == lexicon_dictionary['-'])\n",
    "\n",
    "                desired_answers = np.array(story['outputs'])\n",
    "                input_vec, _, seq_len, _ = prepare_sample([story], lexicon_dictionary['-'], len(lexicon_dictionary))\n",
    "                softmax_output = session.run(softmaxed, feed_dict={\n",
    "                        input_data: input_vec,\n",
    "                        sequence_length: seq_len\n",
    "                })\n",
    "\n",
    "                softmax_output = np.squeeze(softmax_output)\n",
    "                given_answers = np.argmax(softmax_output[target_mask], axis=1)\n",
    "\n",
    "                answers_cursor = 0\n",
    "                for question_indx in questions_indecies:\n",
    "                    question_grade = []\n",
    "                    targets_cursor = question_indx + 1\n",
    "                    while targets_cursor < len(astory) and astory[targets_cursor] == lexicon_dictionary['-']:\n",
    "                        question_grade.append(given_answers[answers_cursor] == desired_answers[answers_cursor])\n",
    "                        answers_cursor += 1\n",
    "                        targets_cursor += 1\n",
    "                    results.append(np.prod(question_grade))\n",
    "\n",
    "                counter += 1\n",
    "                llprint(\"\\r%s ... %d/%d\" % (task_name, counter, len(test_data)))\n",
    "\n",
    "            error_rate = 1. - np.mean(results)\n",
    "            tasks_results[task_number] = error_rate\n",
    "            llprint(\"\\r%s ... %.3f%% Error Rate.\\n\" % (task_name, error_rate * 100))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"%-27s%-27s%s\" % (\"Task\", \"Result\", \"Paper's Mean\"))\n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        paper_means = {\n",
    "            '1': '9.0 ± 12.6%',\n",
    "            '2': '39.2 ± 20.5%',\n",
    "            '3': '39.6 ± 16.4%',\n",
    "            '4': '0.4 ± 0.7%',\n",
    "            '5': '1.5 ± 1.0%',\n",
    "            '6': '6.9 ± 7.5%',\n",
    "            '7': '9.8 ± 7.0%',\n",
    "            '8': '5.5 ± 5.9%',\n",
    "            '9': '7.7 ± 8.3%',\n",
    "            '10': '9.6 ± 11.4%',\n",
    "            '11':'3.3 ± 5.7%',\n",
    "            '12': '5.0 ± 6.3%',\n",
    "            '13': '3.1 ± 3.6%',\n",
    "            '14': '11.0 ± 7.5%',\n",
    "            '15': '27.2 ± 20.1%',\n",
    "            '16': '53.6 ± 1.9%',\n",
    "            '17': '32.4 ± 8.0%',\n",
    "            '18': '4.2 ± 1.8%',\n",
    "            '19': '64.6 ± 37.4%',\n",
    "            '20': '0.0 ± 0.1%', \n",
    "            'mean': '16.7 ± 7.6%', \n",
    "            'fail': '11.2 ± 5.4'\n",
    "        }\n",
    "        for k in range(20):\n",
    "            task_id = str(k + 1)\n",
    "            task_result = \"%.2f%%\" % (tasks_results[task_id] * 100)\n",
    "            print(\"%-27s%-27s%s\" % (tasks_names[task_id], task_result, paper_means[task_id]))\n",
    "            \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        all_tasks_results = [v for _,v in tasks_results.items()]\n",
    "        results_mean = \"%.2f%%\" % (np.mean(all_tasks_results) * 100)\n",
    "        failed_count = \"%d\" % (np.sum(np.array(all_tasks_results) > 0.05))\n",
    "\n",
    "        print(\"%-27s%-27s%s\" % (\"Mean Err.\", results_mean, paper_means['mean']))\n",
    "        print(\"%-27s%-27s%s\" % (\"Failed (err. > 5%)\", failed_count, paper_means['fail']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
