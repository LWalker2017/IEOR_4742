{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "IEOR4742_HW6_Problem1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LWalker2017/IEOR_4742/blob/master/IEOR4742_HW6_Problem1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjWzB1-Q1jNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 # GPU Version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA4JSrF62aZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/data.zip -d /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcUG0Ssc2Jf9",
        "colab_type": "code",
        "outputId": "6f0fef57-b7a4-4246-a8a4-f3e937c30330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import getopt\n",
        "import shutil\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "\n",
        "import memory_operations as op\n",
        "\n",
        "# check tensorflow version\n",
        "print(tf.__version__)\n",
        "# Confirm tensorflow can see the GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNvfJqVH1er4",
        "colab_type": "code",
        "outputId": "ff37881b-7c54-43bf-e392-7a0b4126f693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def load(path):\n",
        "    return pickle.load(open(path, 'rb'))\n",
        "\n",
        "def onehot(index, size):\n",
        "    vec = np.zeros(size, dtype=np.float32)\n",
        "    index = int(index)\n",
        "    vec[index] = 1.0\n",
        "    return vec\n",
        "\n",
        "def prepare_sample(sample, target_code, word_space_size):\n",
        "    \n",
        "    \"\"\"\n",
        "    prepares the input/output sequence of a sample story by encoding it\n",
        "    into one-hot vectors and generates the necessary loss weights\n",
        "    \"\"\"\n",
        "    \n",
        "    input_vec = np.array(sample[0]['inputs'], dtype=np.float32)\n",
        "    output_vec = np.array(sample[0]['inputs'], dtype=np.float32)\n",
        "    \n",
        "    seq_len = input_vec.shape[0]\n",
        "    weights_vec = np.zeros(seq_len, dtype=np.float32)\n",
        "\n",
        "    target_mask = (input_vec == target_code)\n",
        "    output_vec[target_mask] = sample[0]['outputs']\n",
        "    weights_vec[target_mask] = 1.0\n",
        "\n",
        "    input_vec = np.array([onehot(code, word_space_size) for code in input_vec])\n",
        "    output_vec = np.array([onehot(code, word_space_size) for code in output_vec])\n",
        "\n",
        "    return (\n",
        "        np.reshape(input_vec, (-1, word_space_size)),\n",
        "        np.reshape(output_vec, (-1, word_space_size)),\n",
        "        seq_len,\n",
        "        np.reshape(weights_vec, (-1, 1))\n",
        "    )\n",
        "\n",
        "# task_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "#get the current path of our code\n",
        "task_dir = Path().resolve()\n",
        "llprint(\"Loading Data ... \")\n",
        "lexicon_dict = load(os.path.join(task_dir, \"data/en-10k/lexicon-dict.pkl\"))\n",
        "data = load(os.path.join(task_dir, \"data/en-10k/train/train.pkl\"))\n",
        "llprint(\"Done!\\n\")\n",
        "\n",
        "# the model parameters\n",
        "\n",
        "# memory size NxW\n",
        "N = 128 #256\n",
        "W = 32  #64 \n",
        "\n",
        "# number of read heads\n",
        "R = 2   #4    # memory parameters\n",
        "\n",
        "X = 159   # input size\n",
        "Y = 159   # output size\n",
        "\n",
        "NN = 256  # controller's network output size\n",
        "\n",
        "# setting up zeta\n",
        "zeta_size = R*W + 3*W + 5*R + 3\n",
        "\n",
        "# training parameters, number of iterations\n",
        "iterations = 80000   #10000\n",
        "\n",
        "# in case of using RMSProp\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "def network(step_input, state):\n",
        "    \n",
        "    \"\"\"\n",
        "    defines the recurrent neural network operation\n",
        "    \"\"\"\n",
        "    \n",
        "    global NN\n",
        "    step_input = tf.expand_dims(step_input, 0)\n",
        "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(NN)\n",
        "\n",
        "    return lstm_cell(step_input, state)\n",
        "\n",
        "# START: Computational Graph\n",
        "graph = tf.Graph()\n",
        "\n",
        "\n",
        "with graph.as_default():\n",
        "    \n",
        "    # optimizer\n",
        "    # optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum)\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "    # placeholders\n",
        "    input_data = tf.placeholder(tf.float32, [None, X])\n",
        "    target_output = tf.placeholder(tf.float32, [None, Y])\n",
        "    loss_weights = tf.placeholder(tf.float32, [None, 1])\n",
        "    sequence_length = tf.placeholder(tf.int32)\n",
        "\n",
        "    initial_nn_state = tf.nn.rnn_cell.BasicLSTMCell(NN).zero_state(1, tf.float32)\n",
        "\n",
        "    empty_unpacked_inputs = tf.TensorArray(tf.float32, sequence_length)\n",
        "    unpacked_inputs = empty_unpacked_inputs.unstack(input_data)\n",
        "    outputs_container = tf.TensorArray(tf.float32, sequence_length)  # accumulates the step outputs\n",
        "    t = tf.constant(0, dtype=tf.int32)\n",
        "\n",
        "    def step_op(time, memory_state, controller_state, inputs, outputs):\n",
        "        \n",
        "        \"\"\"\n",
        "        defines the operation of one step of the sequence\n",
        "        \"\"\"\n",
        "        global N, W, R\n",
        "\n",
        "        step_input = inputs.read(time)\n",
        "        M, u, p, L, wr, ww, r = memory_state\n",
        "\n",
        "        with tf.variable_scope('controller'):\n",
        "\n",
        "            std = lambda input_size: np.minimum(0.01, np.sqrt(2./ input_size))\n",
        "            \n",
        "            Xt = tf.concat(values=[step_input, tf.reshape(r, [-1])],axis=0,)\n",
        "            #Xt = tf.concat(0, [step_input, tf.reshape(r, [-1])])\n",
        "\n",
        "            nn_output, nn_state = network(Xt, controller_state)\n",
        "\n",
        "            W_zeta = tf.get_variable('W_zeta', [NN, zeta_size], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
        "            #W_zeta = tf.get_variable(\"W_zeta\", [NN, zeta_size], tf.truncated_normal_initializer(stddev=std(NN)))\n",
        "            \n",
        "            #weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
        "            #W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
        "\n",
        "            W_y = tf.get_variable('W_y', [NN, Y], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
        "            #W_y = tf.get_variable(\"W_y\", [NN, Y], tf.truncated_normal_initializer(stddev=std(NN)))\n",
        "            #W_y = tf.get_variable('W_y', [NN, Y], tf.float32, tf.truncated_normal_initializer(stddev=std(NN)))\n",
        "\n",
        "            \n",
        "            pre_output = tf.matmul(nn_output, W_y)\n",
        "            zeta = tf.squeeze(tf.matmul(nn_output, W_zeta))\n",
        "            kr, br, kw, bw, e, v, f, ga, gw, pi = op.parse_interface(zeta, N, W, R)\n",
        "            \n",
        "            # write head operations\n",
        "            u_t = op.ut(u, f, wr, ww)\n",
        "\n",
        "            #a_t = op.at(u_t, N)\n",
        "            cw_t = op.C(M, kw, bw)\n",
        "            a_t = op.at(u_t, N)\n",
        "            ww_t = op.wwt(cw_t, a_t, gw, ga)\n",
        "            M_t = op.Mt(M, ww_t, e, v)\n",
        "            L_t = op.Lt(L, ww_t, p, N)\n",
        "            p_t = op.pt(ww_t, p)\n",
        "\n",
        "            # read heads operations\n",
        "            cr_t = op.C(M_t, kr, br)\n",
        "            wr_t = op.wrt(wr, L_t, cr_t, pi)\n",
        "            r_t = op.rt(M_t, wr_t)\n",
        "\n",
        "            W_r = tf.get_variable('W_r', [W*R, Y], tf.float32, tf.truncated_normal_initializer(stddev=std(W*R)))\n",
        "            flat_rt = tf.reshape(r_t, [-1])\n",
        "            final_output = pre_output + tf.matmul(tf.expand_dims(flat_rt, 0), W_r)\n",
        "            updated_outputs = outputs.write(time, tf.squeeze(final_output))\n",
        "\n",
        "            return time + 1, (M_t, u_t, p_t, L_t, wr_t, ww_t, r_t), nn_state, inputs, updated_outputs\n",
        "    \n",
        "    _, _, _, _, final_outputs = tf.while_loop(\n",
        "        cond = lambda time, *_: time < sequence_length,\n",
        "        body = step_op, loop_vars=(t, op.init_memory(N,W,R), initial_nn_state, unpacked_inputs, outputs_container),\n",
        "        parallel_iterations=32,\n",
        "        swap_memory=True\n",
        "    )\n",
        "\n",
        "    # pack the individual steps outputs into a single (sequence_length x Y) tensor\n",
        "    packed_output = final_outputs.stack()\n",
        "    loss = tf.reduce_mean(loss_weights*tf.nn.softmax_cross_entropy_with_logits(logits=packed_output, labels=target_output))\n",
        "    gradients = optimizer.compute_gradients(loss)\n",
        "    \n",
        "    \n",
        "    # clipping the gradients by value to avoid explosion\n",
        "    for i, (grad, var) in enumerate(gradients):\n",
        "        if grad is not None:\n",
        "            gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n",
        "    apply_grads = optimizer.apply_gradients(gradients)\n",
        "    # END: Computational Graph\n",
        "\n",
        "#     # Reading command line arguments and adapting parameters\n",
        "#     options,_ = getopt.getopt(sys.argv[1:], '', ['iterations='])\n",
        "#     for opt in options:\n",
        "#         iterations = int(opt[1])\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "\n",
        "        #session.run(tf.global_variables_initializer())\n",
        "        session.run(tf.initializers.global_variables())\n",
        "\n",
        "        last_100_losses = []\n",
        "        print(\"\")\n",
        "        for i in range(iterations):\n",
        "\n",
        "            llprint(\"\\r iteration %d/%d\" % (i, iterations))\n",
        "\n",
        "            sample = np.random.choice(data, 1)\n",
        "            input_seq, target_seq, seq_len, weights = prepare_sample(sample, lexicon_dict['-'], 159)\n",
        "\n",
        "            loss_value,_, = session.run([loss, apply_grads], feed_dict={\n",
        "                input_data: input_seq,\n",
        "                target_output: target_seq,\n",
        "                sequence_length: seq_len,\n",
        "                loss_weights: weights\n",
        "            })\n",
        "\n",
        "            last_100_losses.append(loss_value)\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\n\\tAvg. Cross-Entropy Loss: %0.6f\" % (np.mean(last_100_losses)))\n",
        "                last_100_losses = []\n",
        "\n",
        "        model_path = os.path.join(task_dir, 'bAbI-model')\n",
        "        \n",
        "        if os.path.exists(model_path):\n",
        "            shutil.rmtree(model_path)\n",
        "        os.mkdir(model_path)\n",
        "        \n",
        "        tf.train.Saver().save(session, os.path.join(model_path, 'model.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data ... Done!\n",
            "WARNING:tensorflow:From <ipython-input-2-c1d95da8b628>:103: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/memory_operations.py:89: The name tf.cumprod is deprecated. Please use tf.math.cumprod instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-c1d95da8b628>:176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:281: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
            "\n",
            " iteration 0/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.243685\n",
            " iteration 100/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.165832\n",
            " iteration 200/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.153071\n",
            " iteration 300/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.134554\n",
            " iteration 400/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.134949\n",
            " iteration 500/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.112242\n",
            " iteration 600/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.092699\n",
            " iteration 700/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.084747\n",
            " iteration 800/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.067910\n",
            " iteration 900/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.060918\n",
            " iteration 1000/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.055376\n",
            " iteration 1100/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.044493\n",
            " iteration 1200/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.037943\n",
            " iteration 1300/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.036083\n",
            " iteration 1400/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.036066\n",
            " iteration 1500/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.034920\n",
            " iteration 1600/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.031133\n",
            " iteration 1700/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.026022\n",
            " iteration 1800/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.020739\n",
            " iteration 1900/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.022596\n",
            " iteration 2000/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.022368\n",
            " iteration 2100/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.020248\n",
            " iteration 2200/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.017403\n",
            " iteration 2300/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.017543\n",
            " iteration 2400/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.013427\n",
            " iteration 2500/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.012416\n",
            " iteration 2600/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.015792\n",
            " iteration 2700/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.011608\n",
            " iteration 2800/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.010729\n",
            " iteration 2900/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.011284\n",
            " iteration 3000/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.013140\n",
            " iteration 3100/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.010828\n",
            " iteration 3200/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.011358\n",
            " iteration 3300/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.007709\n",
            " iteration 3400/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.010191\n",
            " iteration 3500/80000\n",
            "\tAvg. Cross-Entropy Loss: 0.006685\n",
            " iteration 3504/80000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvoj9oLg22Fi",
        "colab_type": "code",
        "outputId": "fe074d46-c9f7-42b0-ed21-59d1bcf6cfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!zip -r /content/bAbI-model.zip /content/bAbI-model/\n",
        "# !zip -r /content/file.zip /content/Folder_To_Zip\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/file.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/bAbI-model/ (stored 0%)\n",
            "  adding: content/bAbI-model/model.ckpt.meta (deflated 91%)\n",
            "  adding: content/bAbI-model/checkpoint (deflated 47%)\n",
            "  adding: content/bAbI-model/model.ckpt.data-00000-of-00001 (deflated 6%)\n",
            "  adding: content/bAbI-model/model.ckpt.index (deflated 40%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Fp_kLw9rzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! rm -rf /content/bAbI-model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzKw-9HoBmpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}