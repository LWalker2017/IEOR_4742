{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From <ipython-input-1-3b538f3eacaf>:10: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:297: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../HW2/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../HW2/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../HW2/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../HW2/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../HW2/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST data image of shape 28*28=784\n",
    "#mnist_size = 784\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Define the variables for the generator, we will use them to build layers later\n",
    "# -------------------\n",
    "size_g_w1 = 100\n",
    "size_g_b1 = 128\n",
    "# A good way to decide the std for initializing the weights\n",
    "w1_std = 1.0/tf.sqrt(size_g_w1/2.0)\n",
    "\n",
    "G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "size_g_w2 = 128\n",
    "size_g_b2 = 784\n",
    "w2_std = 1.0/tf.sqrt(size_g_w2/2.0)\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "# theta_G and theta_D will be feeded to different optimizers later as \"var_list\", \n",
    "# since currently we have two networks instead of one now.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# ====================\n",
    "# Discriminator\n",
    "# Define the variables for the discriminator\n",
    "# --------------------\n",
    "size_d_w1 = 784\n",
    "size_d_b1 = 128\n",
    "w1_std = 1.0/tf.sqrt(size_d_w1/2.0)\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "size_d_w2 = 128\n",
    "size_d_b2 = 1\n",
    "w2_std = 1.0/tf.sqrt(size_d_w2/2.0)\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    # randomly generate samples for generator\n",
    "    return np.random.uniform(-1.0, 1.0, size = [m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faciliate the path defining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "#get the current path of our code\n",
    "code_dir = Path().resolve()\n",
    "#create output_dir within the same path\n",
    "output_dir = os.path.join(code_dir, 'outputGANs/')\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GNN with defined vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "G_sample, _ = generator(Z)\n",
    "# The result of discriminator of real and fake samples\n",
    "_, D_logit_real = discriminator(X)\n",
    "_, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# generator loss \n",
    "# the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "# tf.ones_like returns a tensor with all elements set to 1\n",
    "# 0 represent fake and 1 means real\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# discriminator loss \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "D_loss: 2.338\n",
      "G_loss: 1.067\n",
      "\n",
      "iteration: 1000\n",
      "D_loss: 0.005836\n",
      "G_loss: 8.6\n",
      "\n",
      "iteration: 2000\n",
      "D_loss: 0.1089\n",
      "G_loss: 4.554\n",
      "\n",
      "iteration: 3000\n",
      "D_loss: 0.07926\n",
      "G_loss: 5.062\n",
      "\n",
      "iteration: 4000\n",
      "D_loss: 0.1557\n",
      "G_loss: 4.866\n",
      "\n",
      "iteration: 5000\n",
      "D_loss: 0.1979\n",
      "G_loss: 4.893\n",
      "\n",
      "iteration: 6000\n",
      "D_loss: 0.3132\n",
      "G_loss: 5.187\n",
      "\n",
      "iteration: 7000\n",
      "D_loss: 0.3319\n",
      "G_loss: 3.948\n",
      "\n",
      "iteration: 8000\n",
      "D_loss: 0.4715\n",
      "G_loss: 2.936\n",
      "\n",
      "iteration: 9000\n",
      "D_loss: 0.7071\n",
      "G_loss: 3.06\n",
      "\n",
      "iteration: 10000\n",
      "D_loss: 0.5869\n",
      "G_loss: 2.779\n",
      "\n",
      "iteration: 11000\n",
      "D_loss: 0.6018\n",
      "G_loss: 3.297\n",
      "\n",
      "iteration: 12000\n",
      "D_loss: 0.5772\n",
      "G_loss: 2.338\n",
      "\n",
      "iteration: 13000\n",
      "D_loss: 0.6994\n",
      "G_loss: 2.29\n",
      "\n",
      "iteration: 14000\n",
      "D_loss: 0.4889\n",
      "G_loss: 2.372\n",
      "\n",
      "iteration: 15000\n",
      "D_loss: 0.7328\n",
      "G_loss: 2.359\n",
      "\n",
      "iteration: 16000\n",
      "D_loss: 0.6246\n",
      "G_loss: 2.289\n",
      "\n",
      "iteration: 17000\n",
      "D_loss: 0.6857\n",
      "G_loss: 2.323\n",
      "\n",
      "iteration: 18000\n",
      "D_loss: 0.6863\n",
      "G_loss: 2.347\n",
      "\n",
      "iteration: 19000\n",
      "D_loss: 0.6599\n",
      "G_loss: 2.385\n",
      "\n",
      "iteration: 20000\n",
      "D_loss: 0.6845\n",
      "G_loss: 2.381\n",
      "\n",
      "iteration: 21000\n",
      "D_loss: 0.6198\n",
      "G_loss: 2.073\n",
      "\n",
      "iteration: 22000\n",
      "D_loss: 0.6627\n",
      "G_loss: 2.319\n",
      "\n",
      "iteration: 23000\n",
      "D_loss: 0.7361\n",
      "G_loss: 2.313\n",
      "\n",
      "iteration: 24000\n",
      "D_loss: 0.7328\n",
      "G_loss: 2.373\n",
      "\n",
      "iteration: 25000\n",
      "D_loss: 0.6147\n",
      "G_loss: 2.32\n",
      "\n",
      "iteration: 26000\n",
      "D_loss: 0.6091\n",
      "G_loss: 2.449\n",
      "\n",
      "iteration: 27000\n",
      "D_loss: 0.5451\n",
      "G_loss: 2.307\n",
      "\n",
      "iteration: 28000\n",
      "D_loss: 0.6158\n",
      "G_loss: 2.306\n",
      "\n",
      "iteration: 29000\n",
      "D_loss: 0.608\n",
      "G_loss: 2.483\n",
      "\n",
      "iteration: 30000\n",
      "D_loss: 0.6543\n",
      "G_loss: 2.254\n",
      "\n",
      "iteration: 31000\n",
      "D_loss: 0.6518\n",
      "G_loss: 2.591\n",
      "\n",
      "iteration: 32000\n",
      "D_loss: 0.7223\n",
      "G_loss: 2.12\n",
      "\n",
      "iteration: 33000\n",
      "D_loss: 0.7864\n",
      "G_loss: 2.106\n",
      "\n",
      "iteration: 34000\n",
      "D_loss: 0.6828\n",
      "G_loss: 2.42\n",
      "\n",
      "iteration: 35000\n",
      "D_loss: 0.7537\n",
      "G_loss: 2.681\n",
      "\n",
      "iteration: 36000\n",
      "D_loss: 0.682\n",
      "G_loss: 2.257\n",
      "\n",
      "iteration: 37000\n",
      "D_loss: 0.6765\n",
      "G_loss: 2.407\n",
      "\n",
      "iteration: 38000\n",
      "D_loss: 0.6225\n",
      "G_loss: 2.177\n",
      "\n",
      "iteration: 39000\n",
      "D_loss: 0.7172\n",
      "G_loss: 2.404\n",
      "\n",
      "iteration: 40000\n",
      "D_loss: 0.6031\n",
      "G_loss: 2.789\n",
      "\n",
      "iteration: 41000\n",
      "D_loss: 0.5666\n",
      "G_loss: 2.343\n",
      "\n",
      "iteration: 42000\n",
      "D_loss: 0.5018\n",
      "G_loss: 2.041\n",
      "\n",
      "iteration: 43000\n",
      "D_loss: 0.615\n",
      "G_loss: 2.448\n",
      "\n",
      "iteration: 44000\n",
      "D_loss: 0.6103\n",
      "G_loss: 2.442\n",
      "\n",
      "iteration: 45000\n",
      "D_loss: 0.5989\n",
      "G_loss: 2.733\n",
      "\n",
      "iteration: 46000\n",
      "D_loss: 0.6198\n",
      "G_loss: 2.48\n",
      "\n",
      "iteration: 47000\n",
      "D_loss: 0.619\n",
      "G_loss: 2.002\n",
      "\n",
      "iteration: 48000\n",
      "D_loss: 0.6591\n",
      "G_loss: 2.29\n",
      "\n",
      "iteration: 49000\n",
      "D_loss: 0.6288\n",
      "G_loss: 2.711\n",
      "\n",
      "iteration: 50000\n",
      "D_loss: 0.5637\n",
      "G_loss: 2.125\n",
      "\n",
      "iteration: 51000\n",
      "D_loss: 0.5386\n",
      "G_loss: 2.706\n",
      "\n",
      "iteration: 52000\n",
      "D_loss: 0.609\n",
      "G_loss: 2.618\n",
      "\n",
      "iteration: 53000\n",
      "D_loss: 0.609\n",
      "G_loss: 2.237\n",
      "\n",
      "iteration: 54000\n",
      "D_loss: 0.6206\n",
      "G_loss: 2.397\n",
      "\n",
      "iteration: 55000\n",
      "D_loss: 0.7321\n",
      "G_loss: 2.235\n",
      "\n",
      "iteration: 56000\n",
      "D_loss: 0.5105\n",
      "G_loss: 2.492\n",
      "\n",
      "iteration: 57000\n",
      "D_loss: 0.5937\n",
      "G_loss: 2.263\n",
      "\n",
      "iteration: 58000\n",
      "D_loss: 0.6237\n",
      "G_loss: 2.547\n",
      "\n",
      "iteration: 59000\n",
      "D_loss: 0.6628\n",
      "G_loss: 2.387\n",
      "\n",
      "iteration: 60000\n",
      "D_loss: 0.5104\n",
      "G_loss: 2.222\n",
      "\n",
      "iteration: 61000\n",
      "D_loss: 0.5925\n",
      "G_loss: 2.412\n",
      "\n",
      "iteration: 62000\n",
      "D_loss: 0.5667\n",
      "G_loss: 2.628\n",
      "\n",
      "iteration: 63000\n",
      "D_loss: 0.5566\n",
      "G_loss: 2.177\n",
      "\n",
      "iteration: 64000\n",
      "D_loss: 0.5464\n",
      "G_loss: 2.266\n",
      "\n",
      "iteration: 65000\n",
      "D_loss: 0.5228\n",
      "G_loss: 2.422\n",
      "\n",
      "iteration: 66000\n",
      "D_loss: 0.5685\n",
      "G_loss: 2.232\n",
      "\n",
      "iteration: 67000\n",
      "D_loss: 0.5281\n",
      "G_loss: 2.284\n",
      "\n",
      "iteration: 68000\n",
      "D_loss: 0.451\n",
      "G_loss: 2.577\n",
      "\n",
      "iteration: 69000\n",
      "D_loss: 0.5671\n",
      "G_loss: 2.485\n",
      "\n",
      "iteration: 70000\n",
      "D_loss: 0.5056\n",
      "G_loss: 2.654\n",
      "\n",
      "iteration: 71000\n",
      "D_loss: 0.5089\n",
      "G_loss: 2.342\n",
      "\n",
      "iteration: 72000\n",
      "D_loss: 0.469\n",
      "G_loss: 2.348\n",
      "\n",
      "iteration: 73000\n",
      "D_loss: 0.5491\n",
      "G_loss: 2.583\n",
      "\n",
      "iteration: 74000\n",
      "D_loss: 0.4296\n",
      "G_loss: 2.351\n",
      "\n",
      "iteration: 75000\n",
      "D_loss: 0.5679\n",
      "G_loss: 2.828\n",
      "\n",
      "iteration: 76000\n",
      "D_loss: 0.557\n",
      "G_loss: 2.493\n",
      "\n",
      "iteration: 77000\n",
      "D_loss: 0.5594\n",
      "G_loss: 2.305\n",
      "\n",
      "iteration: 78000\n",
      "D_loss: 0.475\n",
      "G_loss: 2.69\n",
      "\n",
      "iteration: 79000\n",
      "D_loss: 0.4382\n",
      "G_loss: 2.433\n",
      "\n",
      "iteration: 80000\n",
      "D_loss: 0.4565\n",
      "G_loss: 2.518\n",
      "\n",
      "iteration: 81000\n",
      "D_loss: 0.5708\n",
      "G_loss: 2.348\n",
      "\n",
      "iteration: 82000\n",
      "D_loss: 0.5915\n",
      "G_loss: 2.504\n",
      "\n",
      "iteration: 83000\n",
      "D_loss: 0.5339\n",
      "G_loss: 2.705\n",
      "\n",
      "iteration: 84000\n",
      "D_loss: 0.5557\n",
      "G_loss: 2.512\n",
      "\n",
      "iteration: 85000\n",
      "D_loss: 0.6253\n",
      "G_loss: 2.477\n",
      "\n",
      "iteration: 86000\n",
      "D_loss: 0.6362\n",
      "G_loss: 2.364\n",
      "\n",
      "iteration: 87000\n",
      "D_loss: 0.4996\n",
      "G_loss: 2.145\n",
      "\n",
      "iteration: 88000\n",
      "D_loss: 0.5409\n",
      "G_loss: 2.878\n",
      "\n",
      "iteration: 89000\n",
      "D_loss: 0.5139\n",
      "G_loss: 2.389\n",
      "\n",
      "iteration: 90000\n",
      "D_loss: 0.4325\n",
      "G_loss: 2.542\n",
      "\n",
      "iteration: 91000\n",
      "D_loss: 0.5957\n",
      "G_loss: 2.383\n",
      "\n",
      "iteration: 92000\n",
      "D_loss: 0.5568\n",
      "G_loss: 2.423\n",
      "\n",
      "iteration: 93000\n",
      "D_loss: 0.5476\n",
      "G_loss: 2.198\n",
      "\n",
      "iteration: 94000\n",
      "D_loss: 0.6093\n",
      "G_loss: 1.876\n",
      "\n",
      "iteration: 95000\n",
      "D_loss: 0.4476\n",
      "G_loss: 2.313\n",
      "\n",
      "iteration: 96000\n",
      "D_loss: 0.4963\n",
      "G_loss: 2.401\n",
      "\n",
      "iteration: 97000\n",
      "D_loss: 0.4926\n",
      "G_loss: 2.701\n",
      "\n",
      "iteration: 98000\n",
      "D_loss: 0.5495\n",
      "G_loss: 2.771\n",
      "\n",
      "iteration: 99000\n",
      "D_loss: 0.5195\n",
      "G_loss: 2.503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "# the dimension of the random samples\n",
    "z_dim = 100\n",
    "result_freq = 1000\n",
    "# plot generators' output every figure_iter step\n",
    "figure_iter = 1000\n",
    "max_iter = 100000\n",
    "size1 = 5\n",
    "size2 = 5\n",
    "i = 0\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    \n",
    "    if iter % figure_iter == 0:\n",
    "        \n",
    "        # G_sample is a sample from the generator\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_z(size1*size2, z_dim)})\n",
    "\n",
    "        fig1 = plot_sample(samples, size1, size2)\n",
    "        plt.savefig(output_dir + 'GANs' + str(i) + '.png', bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig1)\n",
    "\n",
    "    batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    _, discriminator_loss = sess.run([D_solver, D_loss], feed_dict={X: batch_xs, Z: sample_z(batch_size, z_dim)})\n",
    "    _, generator_loss     = sess.run([G_solver, G_loss], feed_dict={Z: sample_z(batch_size, z_dim)})\n",
    "\n",
    "    if iter % result_freq == 0:\n",
    "        \n",
    "        print('iteration: {}'.format(iter))\n",
    "        print('D_loss: {:0.4}'.format(discriminator_loss))\n",
    "        print('G_loss: {:0.4}'.format(generator_loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
