{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(x, k):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpooling(x,k):\n",
    "    return tf.nn.max_unpool(x, ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_para():\n",
    "    #first layer of fully_connected of generator\n",
    "    size_g_w1=[1,1024]\n",
    "    size_g_b1=1024\n",
    "\n",
    "    w1_std= 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "    G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "    #second layer of fully_connected of generator\n",
    "    size_g_w2=[1024,32*32*64]\n",
    "    size_g_b2=32*32*64\n",
    "\n",
    "    w2_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "\n",
    "    #first layer of CNN of generator\n",
    "    size_g_w3=[5,5,64,32]\n",
    "    size_g_b3=32\n",
    "    w3_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W3 = tf.Variable(tf.random_normal(shape=[size_g_w3, size_g_b3], stddev=w3_std))\n",
    "    G_b3 = tf.Variable(tf.zeros(shape=[size_g_b3]))\n",
    "\n",
    "    #second layer of CNN of generator\n",
    "    size_g_w4 = [5,5,32,16]\n",
    "    size_g_b4 = 16\n",
    "    w4_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W4 = tf.Variable(tf.random_normal(shape=[size_g_w4, size_g_b4], stddev=w4_std))\n",
    "    G_b4 = tf.Variable(tf.zeros(shape=[size_g_b4]))\n",
    "\n",
    "    #third layer of CNN of generator\n",
    "    size_g_w5=[5,5,16,8]\n",
    "    size_g_b5=8\n",
    "    w5_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W5 = tf.Variable(tf.random_normal(shape=[size_g_w5, size_g_b5], stddev=w5_std))\n",
    "    G_b5 = tf.Variable(tf.zeros(shape=[size_g_b5]))\n",
    "\n",
    "    #fourth layer of CNN of generator\n",
    "    size_g_w6 = [5,5,8,4]\n",
    "    size_g_b6 = 4\n",
    "    w6_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W6 = tf.Variable(tf.random_normal(shape=[size_g_w6, size_g_b6], stddev=w6_std))\n",
    "    G_b6 = tf.Variable(tf.zeros(shape=[size_g_b6]))\n",
    "\n",
    "    #fifth layer of CNN of generator\n",
    "    size_g_w7 = [5,5,4,1]\n",
    "    size_g_b7 = 1\n",
    "    w7_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    G_W7 = tf.Variable(tf.random_normal(shape=[size_g_w7, size_g_b7], stddev=w7_std))\n",
    "    G_b7 = tf.Variable(tf.zeros(shape=[size_g_b7]))\n",
    "\n",
    "    theta_G = [G_W1, G_W2, G_W3, G_W4, G_W5, G_W6, G_W7, G_b1, G_b2, G_b3, G_b4, G_b5, G_b6, G_b7]\n",
    "    return theta_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def discriminator_para():    \n",
    "    #first layer of CNN of discriminator\n",
    "    size_d_w1 = [5,5,1,4]\n",
    "    size_d_b1 = 4\n",
    "    w1_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "    D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "    #second layer of CNN of discriminator\n",
    "    size_d_w2 = [5,5,4,8]\n",
    "    size_d_b2 = 8\n",
    "    w2_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "    D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "    #third layer of CNN of discriminator\n",
    "    size_d_w3 = [5,5,8,16]\n",
    "    size_d_b3 = 16\n",
    "    w3_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W3 = tf.Variable(tf.random_normal(shape=[size_d_w3,size_d_b3], stddev=w3_std))\n",
    "    D_b3 = tf.Variable(tf.zeros(shape=[size_d_b3]))\n",
    "\n",
    "    #fourth layer of CNN of discriminator\n",
    "    size_d_w4 = [5,5,16,32]\n",
    "    size_d_b4 = 32\n",
    "    w4_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W4 = tf.Variable(tf.random_normal(shape=[size_d_w4,size_d_b4], stddev=w4_std))\n",
    "    D_b4 = tf.Variable(tf.zeros(shape=[size_d_b4]))\n",
    "\n",
    "    #fifth layer of CNN of discriminator\n",
    "    size_d_w5 = [5,5,32,64]\n",
    "    size_d_b5 = 64\n",
    "    w5_std = 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W5 = tf.Variable(tf.random_normal(shape=[size_d_w5,size_d_b5], stddev=w5_std))\n",
    "    D_b5 = tf.Variable(tf.zeros(shape=[size_d_b5]))\n",
    "\n",
    "    #first layer of fully_connected of generator\n",
    "    size_d_w6=[32*32*64, 1024]\n",
    "    size_d_b6=1024\n",
    "    w6_std= 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W6 = tf.Variable(tf.random_normal(shape=[size_d_w6, size_d_b6], stddev=w6_std))\n",
    "    D_b6 = tf.Variable(tf.zeros(shape=[size_d_b6]))\n",
    "\n",
    "    #second layer of fully_connected of generator\n",
    "    size_d_w7=[1024,1]\n",
    "    size_d_b7=1\n",
    "    w7_std= 1.0/tf.sqrt(100/2.0)\n",
    "\n",
    "    D_W7 = tf.Variable(tf.random_normal(shape=[size_d_w7, size_d_b7], stddev=w7_std))\n",
    "    D_b7 = tf.Variable(tf.zeros(shape=[size_d_b7]))\n",
    "\n",
    "\n",
    "    theta_D = [D_W1, D_W2, D_W3, D_W4, D_W5, D_W6, D_W7, D_b1, D_b2, D_b3, D_b4, D_b5, D_b6, D_b7]\n",
    "    \n",
    "    return theta_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    \n",
    "    #the input x is a figure with size (1024,1024,1)\n",
    "    \n",
    "    D_h1 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d(x, D_W1, strides=[1, 1, 1, 1], padding='SAME'), D_b1))\n",
    "    D_p1=pooling(D_h1,2)#size:(512,512,4)\n",
    "    \n",
    "    D_h2 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d(D_p1, D_W2, strides=[1, 1, 1, 1], padding='SAME'), D_b2))\n",
    "    D_p2=pooling(D_h2,2)#size:(256,256,8)\n",
    "    \n",
    "    D_h3 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d(D_p2, D_W3, strides=[1, 1, 1, 1], padding='SAME'), D_b3))\n",
    "    D_p3=pooling(D_h3,2)#size:(128,128,16)\n",
    "    \n",
    "    D_h4 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d(D_p3, D_W4, strides=[1, 1, 1, 1], padding='SAME'), D_b4))\n",
    "    D_p4=pooling(D_h4,2)#size:(64,64,32)\n",
    "    \n",
    "    D_h5 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d(D_p4, D_W5, strides=[1, 1, 1, 1], padding='SAME'), D_b5))\n",
    "    D_p5=pooling(D_h5,2)#size:(32,32,64)\n",
    "    \n",
    "    D_re=tf.reshape(D_p5,shape=[-1,32,32,64])#reshape the 3-dimension matrix to a (32*32*64,1)vector\n",
    "    \n",
    "    D_h6 = tf.nn.relu(tf.matmul(D_re, D_W6) + D_b6)#size:1024\n",
    "    \n",
    "    D_logit = tf.nn.relu(tf.matmul(D_h1, D_W7) + D_b7)#size:1\n",
    "    \n",
    "    \n",
    "    D_prob=tf.nn.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \n",
    "    #the input z is a random noise with size 1\n",
    "    \n",
    "    G_h1=tf.nn.relu(tf.matmul(z, G_W1) + G_b1)#size:1024\n",
    "    \n",
    "    G_h2=tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)#size:32*32*64\n",
    "    \n",
    "    G_re=tf.reshape(G_h2,shape=[32,32,64])#reshape the vector into a (32,32,64) 3-dimension matrix\n",
    "    \n",
    "\n",
    "    G_h3 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d_transpose(G_re, G_W3, strides=[1, 1, 1, 1], padding='SAME'), G_b3))    \n",
    "    G_p3=unpooling(G_h3,2)#size:(64,64,32)\n",
    "    \n",
    "    G_h4 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d_transpose(G_p3, G_W4, strides=[1, 1, 1, 1], padding='SAME'), G_b4))    \n",
    "    G_p4=unpooling(G_h4,2)#size:(128,128,16)\n",
    "    \n",
    "    G_h5 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d_transpose(G_p4, G_W5, strides=[1, 1, 1, 1], padding='SAME'), G_b5))    \n",
    "    G_p5=unpooling(G_h5,2)#size:(256,256,8)\n",
    "    \n",
    "    G_h6 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d_transpose(G_p5, G_W6, strides=[1, 1, 1, 1], padding='SAME'), G_b6))    \n",
    "    G_p6=unpooling(G_h6,2)#size:(512,512,4)\n",
    "    \n",
    "    G_h7 = tf.nn.relu(tf.nn.bias_add(tf.layers.conv2d_transpose(G_p6, G_W7, strides=[1, 1, 1, 1], padding='SAME'), G_b7))    \n",
    "    G_logit=unpooling(G_h7,2)#size:(1024,1024,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, std):\n",
    "    #generate random noise\n",
    "    return np.random.rand(loc=mu,scale=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    # Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "    G_sample, _ = generator(sample_z(mu,std))\n",
    "    # The result of discriminator of real and fake samples\n",
    "    _, D_logit_real = discriminator(X)\n",
    "    _, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "    # generator loss \n",
    "    # the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "    # tf.ones_like returns a tensor with all elements set to 1\n",
    "    # 0 represent fake and 1 means real\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "    # discriminator loss \n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest is start session to train "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
