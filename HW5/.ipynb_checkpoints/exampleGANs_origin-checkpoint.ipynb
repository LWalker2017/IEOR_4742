{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MNIST data image of shape 28*28=784\n",
    "#mnist_size = 784\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Define the variables for the generator, we will use them to build layers later\n",
    "# -------------------\n",
    "size_g_w1 = 100\n",
    "size_g_b1 = 128\n",
    "# A good way to decide the std for initializing the weights\n",
    "w1_std = 1.0/tf.sqrt(size_g_w1/2.0)\n",
    "\n",
    "G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "size_g_w2 = 128\n",
    "size_g_b2 = 784\n",
    "w2_std = 1.0/tf.sqrt(size_g_w2/2.0)\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "# theta_G and theta_D will be feeded to different optimizers later as \"var_list\", \n",
    "# since currently we have two networks instead of one now.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# ====================\n",
    "# Discriminator\n",
    "# Define the variables for the discriminator\n",
    "# --------------------\n",
    "size_d_w1 = 784\n",
    "size_d_b1 = 128\n",
    "w1_std = 1.0/tf.sqrt(size_d_w1/2.0)\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "size_d_w2 = 128\n",
    "size_d_b2 = 1\n",
    "w2_std = 1.0/tf.sqrt(size_d_w2/2.0)\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    # randomly generate samples for generator\n",
    "    return np.random.uniform(-1.0, 1.0, size = [m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faciliate the path defining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "#get the current path of our code\n",
    "code_dir = Path().resolve()\n",
    "#create output_dir within the same path\n",
    "output_dir = os.path.join(code_dir, 'outputGANs/')\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GNN with defined vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "G_sample, _ = generator(Z)\n",
    "# The result of discriminator of real and fake samples\n",
    "_, D_logit_real = discriminator(X)\n",
    "_, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# generator loss \n",
    "# the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "# tf.ones_like returns a tensor with all elements set to 1\n",
    "# 0 represent fake and 1 means real\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# discriminator loss \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "D_loss: 1.617\n",
      "G_loss: 2.95\n",
      "\n",
      "iteration: 1000\n",
      "D_loss: 0.002307\n",
      "G_loss: 8.966\n",
      "\n",
      "iteration: 2000\n",
      "D_loss: 0.0602\n",
      "G_loss: 5.122\n",
      "\n",
      "iteration: 3000\n",
      "D_loss: 0.08666\n",
      "G_loss: 5.902\n",
      "\n",
      "iteration: 4000\n",
      "D_loss: 0.2904\n",
      "G_loss: 5.127\n",
      "\n",
      "iteration: 5000\n",
      "D_loss: 0.262\n",
      "G_loss: 4.854\n",
      "\n",
      "iteration: 6000\n",
      "D_loss: 0.2517\n",
      "G_loss: 5.672\n",
      "\n",
      "iteration: 7000\n",
      "D_loss: 0.2779\n",
      "G_loss: 5.312\n",
      "\n",
      "iteration: 8000\n",
      "D_loss: 0.4214\n",
      "G_loss: 4.143\n",
      "\n",
      "iteration: 9000\n",
      "D_loss: 0.5435\n",
      "G_loss: 3.23\n",
      "\n",
      "iteration: 10000\n",
      "D_loss: 0.4516\n",
      "G_loss: 3.479\n",
      "\n",
      "iteration: 11000\n",
      "D_loss: 0.5126\n",
      "G_loss: 3.444\n",
      "\n",
      "iteration: 12000\n",
      "D_loss: 0.5742\n",
      "G_loss: 3.249\n",
      "\n",
      "iteration: 13000\n",
      "D_loss: 0.6915\n",
      "G_loss: 2.984\n",
      "\n",
      "iteration: 14000\n",
      "D_loss: 0.6246\n",
      "G_loss: 2.59\n",
      "\n",
      "iteration: 15000\n",
      "D_loss: 0.6908\n",
      "G_loss: 2.276\n",
      "\n",
      "iteration: 16000\n",
      "D_loss: 0.5775\n",
      "G_loss: 2.311\n",
      "\n",
      "iteration: 17000\n",
      "D_loss: 0.8712\n",
      "G_loss: 2.169\n",
      "\n",
      "iteration: 18000\n",
      "D_loss: 0.8137\n",
      "G_loss: 2.295\n",
      "\n",
      "iteration: 19000\n",
      "D_loss: 0.5857\n",
      "G_loss: 2.107\n",
      "\n",
      "iteration: 20000\n",
      "D_loss: 0.6169\n",
      "G_loss: 2.464\n",
      "\n",
      "iteration: 21000\n",
      "D_loss: 0.6397\n",
      "G_loss: 2.543\n",
      "\n",
      "iteration: 22000\n",
      "D_loss: 0.7857\n",
      "G_loss: 2.452\n",
      "\n",
      "iteration: 23000\n",
      "D_loss: 0.5468\n",
      "G_loss: 2.71\n",
      "\n",
      "iteration: 24000\n",
      "D_loss: 0.7397\n",
      "G_loss: 2.31\n",
      "\n",
      "iteration: 25000\n",
      "D_loss: 0.7502\n",
      "G_loss: 2.394\n",
      "\n",
      "iteration: 26000\n",
      "D_loss: 0.6536\n",
      "G_loss: 2.228\n",
      "\n",
      "iteration: 27000\n",
      "D_loss: 0.6387\n",
      "G_loss: 2.144\n",
      "\n",
      "iteration: 28000\n",
      "D_loss: 0.6411\n",
      "G_loss: 2.36\n",
      "\n",
      "iteration: 29000\n",
      "D_loss: 0.6397\n",
      "G_loss: 2.446\n",
      "\n",
      "iteration: 30000\n",
      "D_loss: 0.6299\n",
      "G_loss: 2.773\n",
      "\n",
      "iteration: 31000\n",
      "D_loss: 0.6274\n",
      "G_loss: 2.52\n",
      "\n",
      "iteration: 32000\n",
      "D_loss: 0.5656\n",
      "G_loss: 2.655\n",
      "\n",
      "iteration: 33000\n",
      "D_loss: 0.6994\n",
      "G_loss: 3.128\n",
      "\n",
      "iteration: 34000\n",
      "D_loss: 0.5462\n",
      "G_loss: 2.835\n",
      "\n",
      "iteration: 35000\n",
      "D_loss: 0.6457\n",
      "G_loss: 2.876\n",
      "\n",
      "iteration: 36000\n",
      "D_loss: 0.59\n",
      "G_loss: 2.661\n",
      "\n",
      "iteration: 37000\n",
      "D_loss: 0.3842\n",
      "G_loss: 2.704\n",
      "\n",
      "iteration: 38000\n",
      "D_loss: 0.5231\n",
      "G_loss: 2.831\n",
      "\n",
      "iteration: 39000\n",
      "D_loss: 0.5769\n",
      "G_loss: 2.864\n",
      "\n",
      "iteration: 40000\n",
      "D_loss: 0.629\n",
      "G_loss: 2.711\n",
      "\n",
      "iteration: 41000\n",
      "D_loss: 0.5263\n",
      "G_loss: 2.355\n",
      "\n",
      "iteration: 42000\n",
      "D_loss: 0.6052\n",
      "G_loss: 3.094\n",
      "\n",
      "iteration: 43000\n",
      "D_loss: 0.471\n",
      "G_loss: 2.803\n",
      "\n",
      "iteration: 44000\n",
      "D_loss: 0.5022\n",
      "G_loss: 2.833\n",
      "\n",
      "iteration: 45000\n",
      "D_loss: 0.6524\n",
      "G_loss: 3.054\n",
      "\n",
      "iteration: 46000\n",
      "D_loss: 0.4869\n",
      "G_loss: 2.486\n",
      "\n",
      "iteration: 47000\n",
      "D_loss: 0.4454\n",
      "G_loss: 3.003\n",
      "\n",
      "iteration: 48000\n",
      "D_loss: 0.5073\n",
      "G_loss: 2.455\n",
      "\n",
      "iteration: 49000\n",
      "D_loss: 0.5225\n",
      "G_loss: 2.718\n",
      "\n",
      "iteration: 50000\n",
      "D_loss: 0.5102\n",
      "G_loss: 2.845\n",
      "\n",
      "iteration: 51000\n",
      "D_loss: 0.4441\n",
      "G_loss: 2.992\n",
      "\n",
      "iteration: 52000\n",
      "D_loss: 0.5174\n",
      "G_loss: 2.612\n",
      "\n",
      "iteration: 53000\n",
      "D_loss: 0.6812\n",
      "G_loss: 2.854\n",
      "\n",
      "iteration: 54000\n",
      "D_loss: 0.54\n",
      "G_loss: 3.062\n",
      "\n",
      "iteration: 55000\n",
      "D_loss: 0.6399\n",
      "G_loss: 2.56\n",
      "\n",
      "iteration: 56000\n",
      "D_loss: 0.5427\n",
      "G_loss: 2.538\n",
      "\n",
      "iteration: 57000\n",
      "D_loss: 0.5745\n",
      "G_loss: 2.874\n",
      "\n",
      "iteration: 58000\n",
      "D_loss: 0.5556\n",
      "G_loss: 2.825\n",
      "\n",
      "iteration: 59000\n",
      "D_loss: 0.6242\n",
      "G_loss: 2.605\n",
      "\n",
      "iteration: 60000\n",
      "D_loss: 0.4581\n",
      "G_loss: 2.358\n",
      "\n",
      "iteration: 61000\n",
      "D_loss: 0.5975\n",
      "G_loss: 2.671\n",
      "\n",
      "iteration: 62000\n",
      "D_loss: 0.6723\n",
      "G_loss: 2.596\n",
      "\n",
      "iteration: 63000\n",
      "D_loss: 0.5739\n",
      "G_loss: 2.252\n",
      "\n",
      "iteration: 64000\n",
      "D_loss: 0.641\n",
      "G_loss: 2.718\n",
      "\n",
      "iteration: 65000\n",
      "D_loss: 0.5474\n",
      "G_loss: 2.293\n",
      "\n",
      "iteration: 66000\n",
      "D_loss: 0.4624\n",
      "G_loss: 2.496\n",
      "\n",
      "iteration: 67000\n",
      "D_loss: 0.59\n",
      "G_loss: 2.674\n",
      "\n",
      "iteration: 68000\n",
      "D_loss: 0.6485\n",
      "G_loss: 2.625\n",
      "\n",
      "iteration: 69000\n",
      "D_loss: 0.5801\n",
      "G_loss: 2.375\n",
      "\n",
      "iteration: 70000\n",
      "D_loss: 0.4946\n",
      "G_loss: 2.089\n",
      "\n",
      "iteration: 71000\n",
      "D_loss: 0.5393\n",
      "G_loss: 2.408\n",
      "\n",
      "iteration: 72000\n",
      "D_loss: 0.5518\n",
      "G_loss: 2.564\n",
      "\n",
      "iteration: 73000\n",
      "D_loss: 0.6485\n",
      "G_loss: 2.476\n",
      "\n",
      "iteration: 74000\n",
      "D_loss: 0.6024\n",
      "G_loss: 2.706\n",
      "\n",
      "iteration: 75000\n",
      "D_loss: 0.656\n",
      "G_loss: 2.315\n",
      "\n",
      "iteration: 76000\n",
      "D_loss: 0.537\n",
      "G_loss: 2.5\n",
      "\n",
      "iteration: 77000\n",
      "D_loss: 0.4666\n",
      "G_loss: 2.328\n",
      "\n",
      "iteration: 78000\n",
      "D_loss: 0.5139\n",
      "G_loss: 2.845\n",
      "\n",
      "iteration: 79000\n",
      "D_loss: 0.7247\n",
      "G_loss: 2.397\n",
      "\n",
      "iteration: 80000\n",
      "D_loss: 0.5756\n",
      "G_loss: 2.431\n",
      "\n",
      "iteration: 81000\n",
      "D_loss: 0.6498\n",
      "G_loss: 2.348\n",
      "\n",
      "iteration: 82000\n",
      "D_loss: 0.6046\n",
      "G_loss: 2.642\n",
      "\n",
      "iteration: 83000\n",
      "D_loss: 0.6266\n",
      "G_loss: 2.54\n",
      "\n",
      "iteration: 84000\n",
      "D_loss: 0.6309\n",
      "G_loss: 2.465\n",
      "\n",
      "iteration: 85000\n",
      "D_loss: 0.5433\n",
      "G_loss: 2.403\n",
      "\n",
      "iteration: 86000\n",
      "D_loss: 0.5961\n",
      "G_loss: 2.354\n",
      "\n",
      "iteration: 87000\n",
      "D_loss: 0.5926\n",
      "G_loss: 2.704\n",
      "\n",
      "iteration: 88000\n",
      "D_loss: 0.6394\n",
      "G_loss: 3.045\n",
      "\n",
      "iteration: 89000\n",
      "D_loss: 0.5904\n",
      "G_loss: 2.468\n",
      "\n",
      "iteration: 90000\n",
      "D_loss: 0.4979\n",
      "G_loss: 2.431\n",
      "\n",
      "iteration: 91000\n",
      "D_loss: 0.6562\n",
      "G_loss: 2.329\n",
      "\n",
      "iteration: 92000\n",
      "D_loss: 0.4367\n",
      "G_loss: 2.751\n",
      "\n",
      "iteration: 93000\n",
      "D_loss: 0.4823\n",
      "G_loss: 2.444\n",
      "\n",
      "iteration: 94000\n",
      "D_loss: 0.5481\n",
      "G_loss: 2.376\n",
      "\n",
      "iteration: 95000\n",
      "D_loss: 0.5775\n",
      "G_loss: 2.556\n",
      "\n",
      "iteration: 96000\n",
      "D_loss: 0.5878\n",
      "G_loss: 2.542\n",
      "\n",
      "iteration: 97000\n",
      "D_loss: 0.6124\n",
      "G_loss: 2.531\n",
      "\n",
      "iteration: 98000\n",
      "D_loss: 0.5103\n",
      "G_loss: 2.58\n",
      "\n",
      "iteration: 99000\n",
      "D_loss: 0.4993\n",
      "G_loss: 2.929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "# the dimension of the random samples\n",
    "z_dim = 100\n",
    "result_freq = 1000\n",
    "# plot generators' output every figure_iter step\n",
    "figure_iter = 1000\n",
    "max_iter = 100000\n",
    "size1 = 5\n",
    "size2 = 5\n",
    "i = 0\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    \n",
    "    if iter % figure_iter == 0:\n",
    "        \n",
    "        # G_sample is a sample from the generator\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_z(size1*size2, z_dim)})\n",
    "\n",
    "        fig1 = plot_sample(samples, size1, size2)\n",
    "        plt.savefig(output_dir + 'GANs' + str(i) + '.png', bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig1)\n",
    "\n",
    "    batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    _, discriminator_loss = sess.run([D_solver, D_loss], feed_dict={X: batch_xs, Z: sample_z(batch_size, z_dim)})\n",
    "    _, generator_loss     = sess.run([G_solver, G_loss], feed_dict={Z: sample_z(batch_size, z_dim)})\n",
    "\n",
    "    if iter % result_freq == 0:\n",
    "        \n",
    "        print('iteration: {}'.format(iter))\n",
    "        print('D_loss: {:0.4}'.format(discriminator_loss))\n",
    "        print('G_loss: {:0.4}'.format(generator_loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
