{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#222 images data image of shape 512*512=262144\n",
    "#images_size = 262144\n",
    "X = tf.placeholder(tf.float32, shape=[None, 262144])\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Define the variables for the generator, we will use them to build layers later\n",
    "# -------------------\n",
    "size_g_w1 = 100\n",
    "size_g_b1 = 128\n",
    "# A good way to decide the std for initializing the weights\n",
    "w1_std = 1.0/tf.sqrt(size_g_w1/2.0)\n",
    "\n",
    "G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "size_g_w2 = 128\n",
    "size_g_b2 = 262144\n",
    "w2_std = 1.0/tf.sqrt(size_g_w2/2.0)\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "# theta_G and theta_D will be feeded to different optimizers later as \"var_list\", \n",
    "# since currently we have two networks instead of one now.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# ====================\n",
    "# Discriminator\n",
    "# Define the variables for the discriminator\n",
    "# --------------------\n",
    "size_d_w1 = 262144\n",
    "size_d_b1 = 128\n",
    "w1_std = 1.0/tf.sqrt(size_d_w1/2.0)\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "size_d_w2 = 128\n",
    "size_d_b2 = 1\n",
    "w2_std = 1.0/tf.sqrt(size_d_w2/2.0)\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    # randomly generate samples for generator\n",
    "    return np.random.uniform(-1.0, 1.0, size = [m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(512, 512), cmap='gray')\n",
    "        # plt.imshow(sample.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faciliate the path defining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "#get the current path of our code\n",
    "code_dir = Path().resolve()\n",
    "#create output_dir within the same path\n",
    "output_dir = os.path.join(code_dir, 'Problem1_a/')\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use all images with random shuffling for training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(data, num):\n",
    "    '''\n",
    "    Return a total of `num` random samples \n",
    "    '''\n",
    "    #print(len(data))\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = np.array([data[i] for i in idx])\n",
    "\n",
    "    return data_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read image file from given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizePixel1 = 512\n",
    "sizePixel2 = sizePixel1*sizePixel1\n",
    "size_d = 32\n",
    "\n",
    "def read_tensor_from_image_file(path, input_height=sizePixel1, input_width=sizePixel1, input_mean=0, input_std=255):\n",
    "    \n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    file_reader = tf.read_file(path, input_name)\n",
    "    image_reader = tf.image.decode_png(file_reader, channels = 1)\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "    return result \n",
    "\n",
    "\n",
    "nSeries = 222\n",
    "numInSeries = 1\n",
    "nImages = nSeries*numInSeries #222\n",
    "img  = np.zeros((nImages, sizePixel1*sizePixel1))\n",
    "counter = 0\n",
    "\n",
    "#get the current path of our code\n",
    "folder = Path().resolve()\n",
    "\n",
    "for j in range(0,numInSeries):\n",
    "    for i in range(1,nSeries+1):\n",
    "        # print(counter,i,j)\n",
    "        fname = str(i) + '_' + str(j) + '.png'\n",
    "        path = './data/' + fname\n",
    "        orig_img = read_tensor_from_image_file(path)\n",
    "        # vectorize\n",
    "        img[counter] = orig_img.reshape(-1)\n",
    "        \n",
    "        # original size\n",
    "        # img[counter] = orig_img.reshape(sizePixel1,sizePixel1)\n",
    "        counter = counter+1\n",
    "\n",
    "print('Input image shape is:', img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GNN with defined vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "G_sample, _ = generator(Z)\n",
    "# The result of discriminator of real and fake samples\n",
    "_, D_logit_real = discriminator(X)\n",
    "_, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# generator loss \n",
    "# the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "# tf.ones_like returns a tensor with all elements set to 1\n",
    "# 0 represent fake and 1 means real\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# discriminator loss \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 64\n",
    "# the dimension of the random samples\n",
    "z_dim = 100\n",
    "result_freq = 200\n",
    "# plot generators' output every figure_iter step\n",
    "figure_iter = 200\n",
    "max_iter = 20000\n",
    "size1 = 5\n",
    "size2 = 5\n",
    "i = 0\n",
    "\n",
    "discriminator_loss = np.empty(max_iter)\n",
    "generator_loss = np.empty(max_iter)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    \n",
    "    if iter % figure_iter == 0:\n",
    "        \n",
    "        # G_sample is a sample from the generator\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_z(size1*size2, z_dim)})\n",
    "\n",
    "        fig1 = plot_sample(samples, size1, size2)\n",
    "        plt.savefig(output_dir + 'Problem1_a_' + str(i) + '.png', bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig1)\n",
    "\n",
    "    # batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "    batch_xs = next_batch(img, batch_size)\n",
    "\n",
    "    _, discriminator_loss[iter] = sess.run([D_solver, D_loss], feed_dict={X: batch_xs, Z: sample_z(batch_size, z_dim)})\n",
    "    _, generator_loss[iter]     = sess.run([G_solver, G_loss], feed_dict={Z: sample_z(batch_size, z_dim)})\n",
    "\n",
    "    if iter % result_freq == 0:\n",
    "        \n",
    "        print('iteration: {}'.format(iter))\n",
    "        print('D_loss: {:0.4}'.format(discriminator_loss[iter]))\n",
    "        print('G_loss: {:0.4}'.format(generator_loss[iter]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.arange(max_iter), discriminator_loss, 'r-', label='discriminator_loss')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('GANs loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(max_iter), generator_loss, 'b-', label='generator_loss')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('GANs loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
