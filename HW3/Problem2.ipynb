{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "-3z1KDeaevvF",
    "outputId": "f6d28bb5-ffc1-4be4-e4b3-1141231d7023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
      "\u001b[K     |████████████████████████████████| 411.5MB 40kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.1.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.27.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.17.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (45.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.8.0)\n",
      "Installing collected packages: tensorflow-gpu\n",
      "Successfully installed tensorflow-gpu-1.15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tensorflow",
         "tensorflow_core"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.15.0 # GPU Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIcVKh4ietZE"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zAbLqcAsetZG",
    "outputId": "9454b74b-0013-4f48-daca-658f1e8b0493"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "WARNING:tensorflow:From <ipython-input-1-7d4d96583af1>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "#import control_flow_ops\n",
    "#import control_flow_util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Confirm Tensorflow can see the GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJ_kuPHQetZM"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7vid4QtetZN"
   },
   "outputs": [],
   "source": [
    "# The length of window in the pooling layer\n",
    "k = 2\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0005\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6-u_XcVetZS"
   },
   "source": [
    "# Define 2-d Convolution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuuzOmkSetZV"
   },
   "outputs": [],
   "source": [
    "def module_conv2d(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "    Computes a 2 dimentional convolution given the 4d input and filter\n",
    "    input:\n",
    "        x: [batch, in_height, in_width, in_channels]\n",
    "        weight: [filter_height, filter_width, in_channels, out_channels]\n",
    "        bias: [out_channels]\n",
    "    output:\n",
    "        The relu activation of convolution\n",
    "    \"\"\"\n",
    "    print([weight_shape[0], weight_shape[1], weight_shape[2]])\n",
    "    sizeIn = weight_shape[0] * weight_shape[1] * weight_shape[2]\n",
    "    \n",
    "    # initialize weights with data generated from a normal distribution.\n",
    "    # Sometimes, a smaller stddev can improve the accuracy significantly. Take some trials by yourself.\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/sizeIn)**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    # initialize bias with zeros\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    # Specify the stride length to be one in all directions.\n",
    "    # padding='SAME': pad enough so the output has the same dimensions as the input tensor.\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME'), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYf2jw2xetZc"
   },
   "source": [
    "# Define Layer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z49WF7JsetZe"
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    \n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    \n",
    "    return tf.nn.relu(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quN-_3C2etZl"
   },
   "source": [
    "# Define Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3L3xX3UetZn"
   },
   "outputs": [],
   "source": [
    "def conv_batch_normalization(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x: input data to be batch normalized\n",
    "        n_out: the size of the output tensor\n",
    "        phase_train: a boolean tensor. True: update mean and var. False: not update \n",
    "    \"\"\"\n",
    "    #offset: An offset Tensor, often denoted beta in equations, or None. If present, will be added to the normalized tensor.\n",
    "    #scale: A scale Tensor, often denoted gamma in equations, or None. If present, the scale is applied to the normalized tensor.\n",
    "    beta_init = tf.constant_initializer(value=0.0,dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1.0,dtype=tf.float32)\n",
    "    \n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "    \n",
    "    #tf.nn.moments()函数用于计算均值和方差, axes:求解的维度\n",
    "    [batch_mean, batch_var] = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    \n",
    "    \n",
    "    # use an exponential moving average to estimate the population mean and variance during training\n",
    "    # set decay rate to be larger if you have larger size of data\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    [ema_mean, ema_var] = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        #in training episode, train_mean and train_var have to be updated first by tf.control_dependencies, then \n",
    "        #execute the return line\n",
    "        #https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "    [mean, var] = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, beta, gamma, 1, 0.001)\n",
    "    \n",
    "    return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gsxdws5PetZw"
   },
   "source": [
    "# Define Pooling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7L64rwpetZx"
   },
   "outputs": [],
   "source": [
    "def pooling(x, k):\n",
    "    \"\"\"\n",
    "    Extracts the main information of the conv layer by performs the max pooling on the input x.\n",
    "    input:\n",
    "        x: A 4-D Tensor. [batch, in_height, in_width, in_channels]\n",
    "        k: The length of window\n",
    "    \"\"\"\n",
    "    \n",
    "    #value: A 4-D Tensor of the format specified by data_format. That is x in this case.\n",
    "    #ksize: A 1-D int Tensor of 4 elements. The size of the window for each dimension ofinput\n",
    "    #strides: A 1-D int Tensor of 4 elements. The stride of the sliding window for each dimension of input\n",
    "    #padding: A string, either 'VALID' or 'SAME'. Difference of 'VALID' and 'SAME' in tf.nn.max_pool:\n",
    "    #https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8vdYEQJetZ3"
   },
   "source": [
    "# Define Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KKTsgUYetZ4"
   },
   "outputs": [],
   "source": [
    "def inference(x, keep_prob, phase_train):\n",
    "    \"\"\"\n",
    "    define the structure of the whole network\n",
    "    input:\n",
    "        - x: a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "        - keep_prob: The keep_prob of dropout layer\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape the input into Nx28x28x1 (N # of examples & 1 due to Black-White)\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with tf.variable_scope(\"convolutional_layer_1\"):\n",
    "\n",
    "        # convolutional layer with 32 filters and spatial extent e = 5\n",
    "        # this causes in taking an input of volume with depth of 1 and producing an output tensor with 32 channels.\n",
    "        convolutional_1 = module_conv2d(x, [5, 5, 1, 32], [32])\n",
    "        conv1_bn = conv_batch_normalization(convolutional_1, 32, phase_train)\n",
    "        conv1_out = tf.nn.relu(conv1_bn)\n",
    "        # output in passed to max-pooling to be compressed (k=2 non-overlapping).\n",
    "        pooling_1 = pooling(conv1_out, k)\n",
    "\n",
    "    with tf.variable_scope(\"convolutional_layer_2\"):\n",
    "        \n",
    "        # convolutional layer with 64 filters with spatial extent e = 5\n",
    "        # taking an input tensor with depth of 32 and \n",
    "        # producing an output tensor with depth 64\n",
    "        convolutional_2 = module_conv2d(pooling_1, [5, 5, 32, 64], [64])\n",
    "        conv2_bn = conv_batch_normalization(convolutional_2, 64, phase_train)\n",
    "        conv2_out = tf.nn.relu(conv2_bn)\n",
    "        \n",
    "        # output in passed to max-pooling to be compressed (k=2 non-overlapping).\n",
    "        pooling_2 = pooling(conv2_out, k)\n",
    "\n",
    "    with tf.variable_scope(\"convolutional_layer_3\"):\n",
    "        \n",
    "        # convolutional layer with 128 filters with spatial extent e = 5\n",
    "        # taking an input tensor with depth of 64 and \n",
    "        # producing an output tensor with depth 128\n",
    "        convolutional_3 = module_conv2d(pooling_2, [5, 5, 64, 128], [128])\n",
    "        conv3_bn = conv_batch_normalization(convolutional_3, 128, phase_train)\n",
    "        conv3_out = tf.nn.relu(conv3_bn)\n",
    "        \n",
    "        # output in passed to max-pooling to be compressed (k=2 non-overlapping).\n",
    "        pooling_3 = pooling(conv3_out, k)\n",
    "        \n",
    "    with tf.variable_scope(\"convolutional_layer_4\"):\n",
    "        \n",
    "        # convolutional layer with 256 filters with spatial extent e = 5\n",
    "        # taking an input tensor with depth of 128 and \n",
    "        # producing an output tensor with depth 256\n",
    "        convolutional_4 = module_conv2d(pooling_3, [5, 5, 128, 256], [256])\n",
    "        conv4_bn = conv_batch_normalization(convolutional_4, 256, phase_train)\n",
    "        conv4_out = tf.nn.relu(conv4_bn)\n",
    "        \n",
    "        # output in passed to max-pooling to be compressed (k=2 non-overlapping).\n",
    "        pooling_4 = pooling(conv4_out, k)\n",
    "        \n",
    "    with tf.variable_scope(\"fully_connected\"):\n",
    "        \n",
    "        # pass the output of max-pooling into a Fully_Connected layer\n",
    "        # use reshape to flatten the tensor\n",
    "        # We have 64 filters\n",
    "        # To find the height & width after max-pooling:\n",
    "        # roundup((16-5)/2) + 1 = 7\n",
    "        pool_2_flat = tf.reshape(pooling_4, [-1, 2*2*256])\n",
    "        \n",
    "        # after reshaping, use fully-connected layer to compress\n",
    "        # the flattened representation into a hidden layer of size 1,024?\n",
    "        # each feature map has a height & width of 7\n",
    "        fc_1 = layer(pool_2_flat, [2*2*256, 1024], [1024])\n",
    "        \n",
    "        # apply dropout. You may try to add drop out after every pooling layer.\n",
    "        # outputs the input element scaled up by 1/keep_prob\n",
    "        # The scaling is so that the expected sum is unchanged\n",
    "        fc_1_drop = tf.nn.dropout(fc_1, keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(fc_1_drop, [1024, 10], [10])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBcPF7c6etZ9"
   },
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFUFe7ngetZ_"
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)    \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ip1Dd36NetaE"
   },
   "source": [
    "# Define the Optimizer and Training Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Njw-b1letaF"
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    # using Adam Optimizer \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WijRRwUtetaJ"
   },
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fQNAQ2LetaK"
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz9Q75T9etaN"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5gpibWGXetaO",
    "outputId": "fe6f95c2-1bd9-4d7a-a28f-34bdeff69465",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 1]\n",
      "WARNING:tensorflow:From <ipython-input-5-c5a464c3dd15>:10: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "[5, 5, 32]\n",
      "[5, 5, 64]\n",
      "[5, 5, 128]\n",
      "WARNING:tensorflow:From <ipython-input-7-6cbf105b85d6>:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-8-c84abbc09f74>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 0.170034728\n",
      "Validation Error: 0.012199997901916504\n",
      "Epoch: 0002 cost = 0.031018519\n",
      "Validation Error: 0.013999998569488525\n",
      "Epoch: 0003 cost = 0.017833551\n",
      "Validation Error: 0.013000011444091797\n",
      "Epoch: 0004 cost = 0.012272914\n",
      "Validation Error: 0.009199976921081543\n",
      "Epoch: 0005 cost = 0.008466659\n",
      "Validation Error: 0.007600009441375732\n",
      "Epoch: 0006 cost = 0.006972472\n",
      "Validation Error: 0.00959998369216919\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 0007 cost = 0.004610728\n",
      "Validation Error: 0.009000003337860107\n",
      "Epoch: 0008 cost = 0.003978870\n",
      "Validation Error: 0.009000003337860107\n",
      "Epoch: 0009 cost = 0.003605500\n",
      "Validation Error: 0.008599996566772461\n",
      "Epoch: 0010 cost = 0.003065333\n",
      "Validation Error: 0.013999998569488525\n",
      "Epoch: 0011 cost = 0.003571200\n",
      "Validation Error: 0.009199976921081543\n",
      "Epoch: 0012 cost = 0.001718169\n",
      "Validation Error: 0.006600022315979004\n",
      "Epoch: 0013 cost = 0.002234519\n",
      "Validation Error: 0.009199976921081543\n",
      "Epoch: 0014 cost = 0.002341527\n",
      "Validation Error: 0.008599996566772461\n",
      "Epoch: 0015 cost = 0.002214895\n",
      "Validation Error: 0.007600009441375732\n",
      "Epoch: 0016 cost = 0.001655900\n",
      "Validation Error: 0.00700002908706665\n",
      "Epoch: 0017 cost = 0.001617136\n",
      "Validation Error: 0.008800029754638672\n",
      "Epoch: 0018 cost = 0.000953629\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0019 cost = 0.001849758\n",
      "Validation Error: 0.005800008773803711\n",
      "Epoch: 0020 cost = 0.001611949\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0021 cost = 0.000826480\n",
      "Validation Error: 0.0073999762535095215\n",
      "Epoch: 0022 cost = 0.000638730\n",
      "Validation Error: 0.005800008773803711\n",
      "Epoch: 0023 cost = 0.001113893\n",
      "Validation Error: 0.007200002670288086\n",
      "Epoch: 0024 cost = 0.000958968\n",
      "Validation Error: 0.0098000168800354\n",
      "Epoch: 0025 cost = 0.001616209\n",
      "Validation Error: 0.0055999755859375\n",
      "Epoch: 0026 cost = 0.000582872\n",
      "Validation Error: 0.00700002908706665\n",
      "Epoch: 0027 cost = 0.000866621\n",
      "Validation Error: 0.007600009441375732\n",
      "Epoch: 0028 cost = 0.000908604\n",
      "Validation Error: 0.006200015544891357\n",
      "Epoch: 0029 cost = 0.000450827\n",
      "Validation Error: 0.007799983024597168\n",
      "Epoch: 0030 cost = 0.001287437\n",
      "Validation Error: 0.009000003337860107\n",
      "Epoch: 0031 cost = 0.001178533\n",
      "Validation Error: 0.0073999762535095215\n",
      "Epoch: 0032 cost = 0.000653690\n",
      "Validation Error: 0.006399989128112793\n",
      "Epoch: 0033 cost = 0.000522820\n",
      "Validation Error: 0.010200023651123047\n",
      "Epoch: 0034 cost = 0.001285097\n",
      "Validation Error: 0.007200002670288086\n",
      "Epoch: 0035 cost = 0.000638619\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0036 cost = 0.000475619\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0037 cost = 0.001711619\n",
      "Validation Error: 0.008599996566772461\n",
      "Epoch: 0038 cost = 0.000521270\n",
      "Validation Error: 0.007799983024597168\n",
      "Epoch: 0039 cost = 0.000621853\n",
      "Validation Error: 0.006200015544891357\n",
      "Epoch: 0040 cost = 0.000205933\n",
      "Validation Error: 0.0059999823570251465\n",
      "Epoch: 0041 cost = 0.000259921\n",
      "Validation Error: 0.006600022315979004\n",
      "Epoch: 0042 cost = 0.000911944\n",
      "Validation Error: 0.007600009441375732\n",
      "Epoch: 0043 cost = 0.001497408\n",
      "Validation Error: 0.00700002908706665\n",
      "Epoch: 0044 cost = 0.000410752\n",
      "Validation Error: 0.0059999823570251465\n",
      "Epoch: 0045 cost = 0.000307670\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0046 cost = 0.000155353\n",
      "Validation Error: 0.009400010108947754\n",
      "Epoch: 0047 cost = 0.000789623\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0048 cost = 0.001591605\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0049 cost = 0.000429039\n",
      "Validation Error: 0.0067999958992004395\n",
      "Epoch: 0050 cost = 0.000468468\n",
      "Validation Error: 0.008000016212463379\n",
      "Optimization Done\n",
      "Test Accuracy: 0.9932\n",
      "Execution time (seconds) was 365.039\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #please, make sure you changed for your own path \n",
    "    log_files_path = './logs/CNNs_BN/'\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"MNIST_convoultional_model\"):\n",
    "            #neural network definition\n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # MNIST data image of shape 28*28=784\n",
    "            x = tf.placeholder(\"float\", [None, 784]) \n",
    "            # 0-9 digits recognition\n",
    "            y = tf.placeholder(\"float\", [None, 10])  \n",
    "            \n",
    "            # dropout probability\n",
    "            keep_prob = tf.placeholder(tf.float32) # dropout probability\n",
    "            # boolean tensor shows the current sess.run is on train or test data\n",
    "            phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x, keep_prob, phase_train)\n",
    "            \n",
    "            cost = loss(output, y)\n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            train_op = training(cost, global_step)\n",
    "            eval_op = evaluate(output, y)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter(log_files_path, sess.graph)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            \n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    \n",
    "                    minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5, phase_train: True})\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5, phase_train: True})/total_batch\n",
    "                \n",
    "                \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:0.9f}\".format(avg_cost))\n",
    "                    \n",
    "                    #probability dropout of 1 during validation\n",
    "                    accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels, keep_prob: 1, phase_train: False})\n",
    "                    print(\"Validation Error:\", (1 - accuracy))\n",
    "                    \n",
    "                    # probability dropout of 0.5 during training\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5, phase_train: True})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                    \n",
    "                    saver.save(sess, log_files_path + 'model-checkpoint', global_step=global_step)\n",
    "                    \n",
    "            print(\"Optimization Done\")\n",
    "                    \n",
    "            accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1, phase_train: False})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Execution time (seconds) was %.3f' % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VgsMC2cetaU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_Problem2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
